# -*- coding: utf-8 -*-
"""HousePricePrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Tfme-MlTgU7uX24w7XWoFCGLPIRP6xI1

House Price Prediction Using Supervised Learning (Regression)
"""

import tensorflow as tf
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sb #another data visualization tool (more advanced and can use correlation matrices)

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer
from imblearn.over_sampling import RandomOverSampler
from sklearn.pipeline import Pipeline #make running lines of code cleaner
from sklearn.compose import ColumnTransformer #bundle pipelines
from sklearn.impute import SimpleImputer #data cleaning and preprocessing

#libraries for regression models
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error
from xgboost import XGBRegressor
from sklearn.neighbors import KNeighborsRegressor

#cross validation for small datasets
from sklearn.model_selection import cross_val_score

#view dataset
df = pd.read_csv("/content/drive/MyDrive/ML data/house-prices-advanced-regression-techniques/train.csv")
df.head()

df.describe() #see stats of data

df.info() #get the different data types and the number of values of each column datak'

#drop columns that have too many null values because they most likely will not have an influence in the output variable
columns_to_drop = ['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature']
df = df.drop(labels=columns_to_drop, axis=1)

# #separate data based on numeric or categorical type (get list of features that are numeric or categorical)
# outputLabel = list(df.columns)[-1]

# cat_features = df.select_dtypes(include = [object]).columns
# num_features = df.select_dtypes(include = [int, float]).drop(labels=outputLabel, axis=1).columns #drop the last label since that is the output variable

# print(cat_features)
# print(num_features)
# print(outputLabel)

#clean data before viewing correlation matrix

# df.info()

#based on info, these columns need to be modified to fill NULL
#LotFrontage(float), MasVnrType(obj), MasVnrArea(float), BsmtQual(obj), BsmtCond(obj), BsmtExposure(obj), BsmtFinType1(obj), BsmtFinType2(obj), Electrical(obj), GarageType(obj), GarageYrBlt(float), GarageFinish(obj), GarageQual(obj), GarageCond(obj)

#get unique values for the columns with NULL
missing_vals = ['LotFrontage', 'MasVnrType', 'MasVnrArea', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Electrical', 'GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual', 'GarageCond']
# for m in missing_vals:
#   print(m, ":")
#   print(df[m].unique())

df_copy = df.copy()

#those with a garage year built equal to NULL drop because we can't fill in a default year, it doesn't make sense to put in a default field
# df_copy.dropna(subset=['GarageYrBlt'], inplace=True)
#drop id because it is just a label for each row
df_copy = df_copy.drop(labels=['Id', 'GarageYrBlt'], axis=1)
#fill MasVnrArea NULL values with 0, LotFrontage with the mean of the current values in the column
#fix all categorical columns to change the nans to become 'None' (the string)
values = {'MasVnrArea': 0.0, 'MasVnrType': 'None', 'BsmtQual': 'None', 'BsmtCond': 'None', 'BsmtExposure': 'None', 'BsmtFinType1': 'None', 'BsmtFinType2': 'None', 'Electrical': 'None', 'GarageType': 'None', 'GarageFinish': 'None', 'GarageQual': 'None', 'GargeCond': 'None'}
df_copy = df_copy.fillna(value=values)

# df_copy.info()

# #graph correlations between features and sale price (output label)
# for features in list(num_features):
#   plt.scatter(df[features], df[outputLabel])
#   plt.xlabel(features)
#   plt.ylabel(outputLabel)
#   plt.show()

plt.figure(figsize=(40, 30))
corr_matrix = sb.heatmap(df.corr(), annot=True)
# plt.show()

#https://www.geeksforgeeks.org/how-to-convert-categorical-string-data-into-numeric-in-python/
#con of encoding strings for ML algorithms: models may think there is a priority system within labels where there actually isn't
  #label encoding: https://www.geeksforgeeks.org/ml-label-encoding-of-datasets-in-python/?ref=lbp (priority, but less variables)
  #one hot encoding: https://www.geeksforgeeks.org/ml-one-hot-encoding-of-datasets-in-python/?ref=lbp (no priority, more variables)

#there is something weird that happens when matplotlib encounters NaN values for categorical data. we are going to replace it with a string 'None'

# for features in list(cat_features):
#   # replace NaN values so they can be processed in matplotlib and pandas for categorical data
#   df[features].fillna('None', inplace=True)
#   tempdf = pd.get_dummies(df[features])
#   # save encoding in dataframe so it can be fed into the ML model later
#   #do here...
#   # print(tempdf)
#   # plt.scatter(df[features], df[outputLabel])
#   # plt.xlabel(features)
#   # plt.ylabel(outputLabel)
#   # plt.show()

#drop data that is irrelevant to output variable

#based on the correlation matrix, I'm going to include the input variables that have a strong correlation of 0.5 or higher (on the positive and negative ends)
# strong_corr = ['OverallQual', 'YearBuilt', 'YearRemodAdd', 'TotalBsmtSF', '1stFlrSF', 'GrLivArea', 'FullBath', 'TotRmsAbvGrd', 'GarageCars', 'GarageArea']
# semi_strong_corr = ['LotFrontage', 'MasVnrArea', 'BsmtFinSF1', '2ndFlrSF', 'Fireplaces', 'GarageYrBlt', 'WoodDeckSF', 'OpenPorchSF']
# some_corr = ['LotArea', 'BsmtUnfSF', 'BsmtFullBath', 'HalfBath']
# relevant_input = df[strong_corr+semi_strong_corr+some_corr].copy()
# other = df[semi_strong_corr].copy()
# output = df[df.columns[-1]]
# print(relevant_input.columns)
# print(other)

#split into training and valid data (test data already given)
output = df_copy[df.columns[-1]]
df_copy = df_copy.drop(labels=['SalePrice'], axis=1)
# df_copy.info()
X_train, X_valid, y_train, y_valid = train_test_split(df_copy, output,
                                                      train_size=0.8, test_size=0.2,
                                                      random_state=0)

#encode qualitative data (would need to fix to do one for training and valid data separately)

#after analyzing the test data, there are som errors in the encoding, so we will merge the training and test data, do the encoding, then split them up
#source: https://stackoverflow.com/questions/44026832/valueerror-number-of-features-of-the-model-must-match-the-input
test_data = pd.read_csv("/content/drive/MyDrive/ML data/house-prices-advanced-regression-techniques/test.csv")
X_train ['label'] = 'train'
test_data['label'] = 'test'

#clean test data before testing
columns_to_drop = ['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature']
test_data = test_data.drop(labels=columns_to_drop, axis=1)
# test_data.dropna(subset=['GarageYrBlt'], inplace=True) #cannot delete a row for test data
test_data = test_data.drop(labels=['GarageYrBlt'], axis=1)
num_var_test = {'LotFrontage': test_data['LotFrontage'].mean().round(2)}
test_data = test_data.fillna(value=num_var_test)
#https://stackoverflow.com/questions/37366717/pandas-print-column-name-with-missing-values
missing = list(test_data.columns[test_data.isnull().any()])
cat_feat = list(test_data.select_dtypes(include=['object']).columns)
values = {}
for m in missing:
  if m in cat_feat:
    values[m] = 'None'
  else:
    values[m] = 0.0
# print(values)
test_data = test_data.fillna(value=values)
# test_data.info()
#save ids for test data
id = test_data['Id']
test_data = test_data.drop(labels=['Id'], axis=1)

#training data cleaning
# X_train = pd.concat([X_train, y_train], axis=1) #concatenate input and output variables together to delete/modify as necessary
#clean train and valid data (preprocess numerical data with nans)
num_var_train = {'LotFrontage': X_train['LotFrontage'].mean().round(2)}
X_train = X_train.fillna(value=num_var_train)

#valid data cleaning
num_var_valid = {'LotFrontage': X_valid['LotFrontage'].mean().round(2)}
X_valid = X_valid.fillna(value=num_var_valid)

concat_df = pd.concat([X_train, test_data]) #concatenate dataframes
# concat_df.info()

#hot encode categorical variables
from sklearn.preprocessing import OneHotEncoder

cat_feat1 = concat_df.select_dtypes(include = [object]).columns
#we don't want to encode the labels column so we will exclude it
cat_feat1 = list(cat_feat1.drop(labels=['label']))
OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)
OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(concat_df[cat_feat1]))
OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[cat_feat1]))

print(OH_cols_train.head())
print(concat_df.index)

OH_cols_train.index = concat_df.index
OH_cols_valid.index = X_valid.index

# print(OH_cols_train.head())

num_X_train = concat_df.drop(cat_feat1, axis=1)
num_X_valid = X_valid.drop(cat_feat1, axis=1)

# print(num_X_train.columns)
# print(len(OH_encoder.get_feature_names_out()))
OH_cols_train.columns = OH_encoder.get_feature_names_out()
# print(OH_cols_train)

OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)
OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)

OH_X_train.columns = OH_X_train.columns.astype('str') #convert all column names to strings
OH_X_valid.columns = OH_X_valid.columns.astype('str')

#separate train and test dataframes
OH_X_test = OH_X_train[OH_X_train['label'] == 'test']
final_train = OH_X_train[OH_X_train['label'] == 'train']
OH_X_test = OH_X_test.drop('label', axis=1)
final_train = final_train.drop('label', axis=1)

# OH_X_test.head()
# final_train.head()

# print(OH_X_train.columns)
# print(OH_cols_train.columns)

#test the valid data now
model = RandomForestRegressor(n_estimators=100, random_state=1)
model.fit(final_train, y_train)
val_pred = model.predict(OH_X_valid)
print(mean_absolute_error(val_pred, y_valid))

#test the test data and submit
# test_data = pd.read_csv("test.csv")

#clean test data before testing
# columns_to_drop = ['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature']
# test_data = test_data.drop(labels=columns_to_drop, axis=1)
# missing_vals = ['LotFrontage', 'MasVnrType', 'MasVnrArea', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Electrical', 'GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual', 'GarageCond']
# test_data.dropna(subset=['GarageYrBlt'], inplace=True)
# id = test_data['Id']
# test_data = test_data.drop(labels=['Id'], axis=1)
# values = {'MasVnrArea': 0.0, 'MasVnrType': 'None', 'BsmtQual': 'None', 'BsmtCond': 'None', 'BsmtExposure': 'None', 'BsmtFinType1': 'None', 'BsmtFinType2': 'None', 'Electrical': 'None', 'GarageType': 'None', 'GarageFinish': 'None', 'GarageQual': 'None', 'GargeCond': 'None'}
# test_data = test_data.fillna(value=values)
# num_var_test = {'LotFrontage': test_data['LotFrontage'].mean().round(2)}
# test_data = test_data.fillna(value=num_var_test)

# cat_feat_test = list(test_data.select_dtypes(include = [object]).columns)
# print(cat_feat_test)
# OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)
# OH_cols_test = pd.DataFrame(OH_encoder.fit_transform(test_data[cat_feat_test]))

# OH_cols_test.index = test_data.index

# num_X_test = test_data.drop(cat_feat_test, axis=1)

# OH_X_test = pd.concat([num_X_test, OH_cols_test], axis=1)

# OH_X_test.columns = OH_X_test.columns.astype('str')

# print(OH_cols_test.columns)

preds_test = model.predict(OH_X_test)

out = pd.DataFrame({'Id': id, 'SalePrice': preds_test})
# print(out)
out.to_csv('submission.csv', index=False)

#score for code above: 16176
#summary of what I did for first test:
#House Price Prediction with dropping variables with significant missing values and no correlation, imputing numerical values with a default value of 0 except LotFrontage(which fills nan values with the mean), and fixing nan categorical values to 'None'. Used one hot encoding for categorical variables. Model used random forest regressor.

#how to improve?
#1) Cross Validation to find best model
#2) use different models: SVM, KClustering, XGBoost, NN, etc.
#3) remove variables with minimal correlation
#4) feature engineering

#using pipelines to make code more readable
original_data = pd.read_csv("/content/drive/MyDrive/ML data/house-prices-advanced-regression-techniques/train.csv")
test_data = pd.read_csv("/content/drive/MyDrive/ML data/house-prices-advanced-regression-techniques/test.csv")

#drop rows with missing target
clean_data = original_data.copy().dropna(axis=0, subset=['SalePrice'])

#drop variables that do not make sense and have no correlation
columns_to_drop = ['Id', 'Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature']
clean_data = clean_data.drop(labels=columns_to_drop, axis=1)

#get numerical, categorical, and output columns
output_col = clean_data.columns[-1]
output = clean_data[output_col]
clean_data = clean_data.drop(labels=output_col, axis=1)
cat_cols = clean_data.select_dtypes(include = [object]).columns
num_cols = clean_data.select_dtypes(include = [int, float]).columns
# print(cat_cols, num_cols, output_col)
# print(output)

#split data into training and validation sets
X_train, X_valid, y_train, y_valid = train_test_split(clean_data, output, train_size=0.8, test_size=0.2, random_state=0)

#preprocess numerical data
numerical_transformer = SimpleImputer(strategy='mean') #we don't know what numerical values might be missing, so lets keep the imputing constant

#preprocess categorical data
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='None')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

#bundle preprocessing
preprocessor = ColumnTransformer(transformers=[
    ('num', numerical_transformer, num_cols),
    ('cat', categorical_transformer, cat_cols)
])

#make model
model2 = RandomForestRegressor(n_estimators=100, random_state=0)

#evaluate data on pipeline
my_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', model2)
])

#find optimal n_estimators (num trees in random forest model) and cv (cross validation splits) to maximize model
def get_score(n_estimators, cross_split):
  cross_model = RandomForestRegressor(n_estimators=n_estimators, random_state=0)
  my_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', cross_model)
  ])
  scores = -1 * cross_val_score(my_pipeline, clean_data, output, cv=cross_split, scoring='neg_mean_absolute_error')
  return scores.mean()

#evaluate on a single iteration
# my_pipeline.fit(X_train, y_train)

#evaluate with cross validation (portion of whole data will be left out to test, rest to train and alternate portions that become test data)
num_trees = [i for i in range(50, 401, 50)]
num_splits = [i for i in range(2, 6)]
results = pd.DataFrame(columns=['num_trees', 'num_splits', 'error'])
# for i in num_trees:
#   for j in num_splits:
#     calc_score = get_score(i, j)
#     temp = pd.DataFrame([[calc_score, i, j]], columns=['error', 'num_trees', 'num_splits'])
#     results = pd.concat([temp, results], ignore_index=True)
# print(results)
for i in num_trees:
  calc_score = get_score(i, 5)
  print(i, calc_score)
#find the best n_estimators and cross split
# scores = -1 * cross_val_score(my_pipeline, original_data, output, cv=5, scoring='neg_mean_absolute_error')
# print(scores.mean())

# val_pred = my_pipeline.predict(X_valid)
# val_score = mean_absolute_error(y_valid, val_pred)
# print(val_score)

#graph the results and choose the best combination to test the predictions
graph = sb.barplot(data=results,x="num_trees",y="error",hue="num_splits",alpha=.6)
plt.show()

#test data and save output for new models here
#model 3: n_estimators = 300, dropped Id, Alley, FirePlaceQC, PoolQC, Fence, MiscFeature (score: 15973)
#model 4: n_estimators = 150, kept all columns except Id (score: 15979)
model = RandomForestRegressor(n_estimators=150, random_state=0)
my_pipeline = Pipeline(steps=[
  ('preprocessor', preprocessor),
  ('model', model)
])

# my_pipeline.fit(X_train, y_train)
# valid_pred = my_pipeline.predict(X_valid)
# val_score = mean_absolute_error(y_valid, valid_pred) 
# print(val_score) 

my_pipeline.fit(clean_data, output)
id = test_data['Id'].copy()
test_pred = my_pipeline.predict(test_data)
out1 = pd.DataFrame({'Id': id, 'SalePrice': test_pred})
# print(out)
out1.to_csv('submission(3-19-23).csv', index=False)

#attempt 5: trying XGBoost (score: 15979)
#attempt 6: XGBoost with modified parameters (score: 14880)
  #https://xgboost.readthedocs.io/en/stable/parameter.html
model4 = XGBRegressor(n_estimators=900, learning_rate=0.05, max_depth=5)
xgb_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', model4)
])
# xgb_pipeline.fit(X_train, y_train)
# val_xgb = xgb_pipeline.predict(X_valid)
# val_score_xgb = mean_absolute_error(y_valid, val_xgb)
# print(val_score_xgb)

xgb_pipeline.fit(clean_data, output)
id = test_data['Id'].copy()
test_pred = xgb_pipeline.predict(test_data)
out1 = pd.DataFrame({'Id': id, 'SalePrice': test_pred})
# print(out)
out1.to_csv('submission(4-1-23).csv', index=False)

#attempt 7: KNearestNeighborsRegressor
#doesn't really work because the data is so sparse: https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-regression
#did not submit this to the competition
model7 = KNeighborsRegressor(n_neighbors=5)
knn_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', model7)
])

knn_pipeline.fit(X_train, y_train)
val_knn = knn_pipeline.predict(X_valid)
val_score_knn = mean_absolute_error(y_valid, val_knn)
print(val_score_knn)

#attempt 8: feature engineering and XGBoost

#using mutual information to determine if there exists a relationship between input variables and output variable (MIdetects any kind of relationship, while correlation can only detect linear relationships)
#we can also use this information to see if there are interaction effects (features that have a low MI but have a relationship with a feature that has a relationship with the output variable)

#functions from Kaggle course:https://www.kaggle.com/code/ryanholbrook/mutual-information
from sklearn.feature_selection import mutual_info_regression

def make_mi_scores(X, y, discrete_features):
    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)
    mi_scores = pd.Series(mi_scores, name="MI Scores", index=X.columns)
    mi_scores = mi_scores.sort_values(ascending=False)
    return mi_scores

def plot_mi_scores(scores):
    scores = scores.sort_values(ascending=True)
    width = np.arange(len(scores))
    ticks = list(scores.index)
    plt.barh(width, scores)
    plt.yticks(width, ticks)
    plt.title("Mutual Information Scores")

#factorize categorical variables (convert to ints)
#actually here I converted categorical variables with one hot encoding
X1 = clean_data.copy()
y1 = output.copy()

# print(X1)

cols = X1.columns.tolist()
num_cols = X1.select_dtypes(include= [int, float]).columns
cat_cols = X1.select_dtypes(include = [object]).columns

def pandaConv(data): #used for one hot encoding
  #add back columns and convert to pandas DF
  data = pd.DataFrame(data.toarray())
  # data.index = X1.index
  data.columns = data.columns.astype('str')
  print(data.head(5))
  return data

def addCols(data, org_cols):
  data = pd.DataFrame(data)
  data.columns = org_cols
  # print(data.head(5))
  return data

def oneHot(data, cat_cols):
  onehot = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
  data = pd.DataFrame(onehot.fit_transform(data[cat_cols]))
  data = pd.DataFrame(data)
  data.columns = onehot.get_feature_names_out()
  # print(data.head(5))
  return data

# def labelEncoder(data): #using label encoding/ordinal encoding (each discrete feature converts str labels to ints)
#   data = pd.DataFrame(data)
#   for colname in data.select_dtypes("object"):
#     data[colname], _ = data[colname].factorize()
#   return data

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='None')),
    ('add', FunctionTransformer(addCols, kw_args={"org_cols": cat_cols})),
    ('onehot', FunctionTransformer(oneHot, kw_args={"cat_cols": cat_cols})),
    # ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False)),
    # ('catadd', FunctionTransformer(oneHot, kw_args={"cat_cols": cat_cols}))
])
numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('add', FunctionTransformer(addCols, kw_args={"org_cols": num_cols}))
])
preprocessor = ColumnTransformer(transformers=[
    ('num', numerical_transformer, num_cols),
    ('cat', categorical_transformer, cat_cols),
])
# https://stackoverflow.com/questions/56338847/how-to-give-column-names-after-one-hot-encoding-with-sklearn
# https://stackoverflow.com/questions/54646709/sklearn-pipeline-get-feature-names-after-onehotencode-in-columntransformer
preprocessing_pipe = Pipeline(steps=[
    ('preprocessor', preprocessor),
])

# https://stackoverflow.com/questions/70933014/how-to-use-columntransformer-to-return-a-dataframe
preprocessor.set_output(transform='pandas') #very important!
preprocessing_pipe.fit(X1)
new_X1 = preprocessing_pipe.transform(X1)
# print(new_X1.head(5))
# print(list(new_X1.columns))

discrete_features = []
for feat in new_X1.columns:
  if "cat" in feat[:3]:
    discrete_features.append(True)
  else:
    discrete_features.append(False)
# print(discrete_features)
mi_scores = make_mi_scores(new_X1, y1, discrete_features)
mi_scores[::3]

#i used one hot encoding to break up the categorical variables and their unique values into their own columns to better see their relationship with the output variable if present
#used simple imputer to fill in NaN values
#mi scores to detect if there is any relationship present between input variables and output variable

plt.figure(dpi=100, figsize=(10,50))
plot_mi_scores(mi_scores)

#find interaction effects between low categorical variables and numerical variables (that make sense)

#types of feature engineering

#mathematical transforms

#reshaping features (like using log to normalize the data)

#counting occurrences (ie. counting total number of different types of accidents for each row)

#building and breaking up features (ie. if house type is blue one story, you can separate into color and levels features or combine features together)

#group transforms (aggregate information across multiple rows grouped by some category) (ie. the average movie income made by genre)